{"version":3,"sources":["../src/kuromojin.js"],"names":[],"mappings":";AACA;;;;;QAagB;QAkBA;;AA5BhB;;;;;;AAFA,IAAM,OAAO,QAAQ,MAAR,CAAP;AACN,IAAM,WAAW,QAAQ,UAAR,CAAX;;AAEN,IAAM,WAAW,wBAAX;AACN,IAAM,uBAAuB,SAAvB,oBAAuB,GAAM;AAC/B,QAAM,cAAc,KAAK,OAAL,CAAa,QAAQ,OAAR,CAAgB,UAAhB,CAAb,CAAd,CADyB;AAE/B,WAAO,KAAK,IAAL,CAAU,WAAV,EAAuB,IAAvB,EAA6B,MAA7B,CAAP,CAF+B;CAAN;;AAK7B,IAAI,aAAa,IAAb;;AAEJ,IAAI,YAAY,KAAZ;AACG,SAAS,YAAT,GAAmE;QAA7C,gEAAU,EAAC,SAAS,sBAAT,kBAAkC;;AACtE,QAAI,UAAJ,EAAgB;AACZ,eAAO,QAAQ,OAAR,CAAgB,UAAhB,CAAP,CADY;KAAhB;AAGA,QAAI,SAAJ,EAAe;AACX,eAAO,SAAS,OAAT,CADI;KAAf;AAGA,gBAAY,IAAZ;;AAPsE,YAStE,CAAS,OAAT,CAAiB,OAAjB,EAA0B,KAA1B,CAAgC,UAAS,GAAT,EAAc,SAAd,EAAyB;AACrD,YAAI,GAAJ,EAAS;AACL,mBAAO,SAAS,MAAT,CAAgB,GAAhB,CAAP,CADK;SAAT;AAGA,qBAAa,SAAb,CAJqD;AAKrD,iBAAS,OAAT,CAAiB,SAAjB,EALqD;KAAzB,CAAhC,CATsE;AAgBtE,WAAO,SAAS,OAAT,CAhB+D;CAAnE;AAkBA,SAAS,QAAT,CAAkB,IAAlB,EAAwB;AAC3B,WAAO,eAAe,IAAf,CAAoB,qBAAa;AACpC,eAAO,UAAU,mBAAV,CAA8B,IAA9B,CAAP,CADoC;KAAb,CAA3B,CAD2B;CAAxB","file":"kuromojin.js","sourcesContent":["// LICENSE : MIT\n\"use strict\";\nconst path = require(\"path\");\nconst kuromoji = require(\"kuromoji\");\nimport Deferred from \"./Deferred\";\nconst deferred = new Deferred();\nconst getNodeModuleDirPath = () => {\n    const kuromojiDir = path.dirname(require.resolve(\"kuromoji\"));\n    return path.join(kuromojiDir, \"..\", \"dict\");\n};\n// cache for tokenizer\nlet _tokenizer = null;\n// lock boolean\nlet isLoading = false;\nexport function getTokenizer(options = {dicPath: getNodeModuleDirPath()}) {\n    if (_tokenizer) {\n        return Promise.resolve(_tokenizer);\n    }\n    if (isLoading) {\n        return deferred.promise;\n    }\n    isLoading = true;\n    // load dict\n    kuromoji.builder(options).build(function(err, tokenizer) {\n        if (err) {\n            return deferred.reject(err);\n        }\n        _tokenizer = tokenizer;\n        deferred.resolve(tokenizer);\n    });\n    return deferred.promise;\n}\nexport function tokenize(text) {\n    return getTokenizer().then(tokenizer => {\n        return tokenizer.tokenizeForSentence(text);\n    });\n}"]}